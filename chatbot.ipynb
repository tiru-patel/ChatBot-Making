# -*- coding: utf-8 -*-
"""ChatBot.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1IVmmE8HLTlw25ER_LHEEqSaU8-Vz0j9A
"""

# Commented out IPython magic to ensure Python compatibility.
# Libraries needed for NLP
import nltk
nltk.download("punkt")
from nltk.stem.lancaster import LancasterStemmer
stemmer = LancasterStemmer()

# Libraries needed for TensorFlow processing 
# %tensorflow_version 1.x
import tensorflow
print(tensorflow.__version__)
import tensorflow as tf
import numpy as np 
import random
import json

import tflearn

from google.colab import files 
files.upload()

# import our chat-bot intents files
with open("intents.json") as json_data:
  intents = json.load(json_data)

intents

words = []
classes = []
documents = []
ignore = ["?"]

# loop through each sentence in the intent's patterns
for intent in intents["intents"]:
  for pattern in intent["patterns"]:
    # tokenize
    w = nltk.word_tokenize(pattern)
    # add word to the words list
    words.extend(w)
    # add word to document 
    documents.append((w,intent['tag']))
    # add tags to our classes 
    if intent['tag'] not in classes:
      classes.append(intent['tag'])

# perform stemming 
words = [stemmer.stem(w.lower()) for w in words if w not in ignore]
words = sorted(list(set(words)))

# remove duplicate classes 
classes = sorted(list(set(classes)))

print(len(documents),"documents")
print(len(classes),"classes",classes)
print(len(words),"unique stemmed words",words)

# create training data
training = []
output = []
# create an empty array for output
output_empty = [0] * len(classes)

# create training set, bag of words for each sentence 
for doc in documents:
  bag = []
  pattern_words = doc[0]
  pattern_words = [stemmer.stem(word.lower()) for word in pattern_words]
  # create bag of words array
  for w in words:
    bag.append(1) if w in pattern_words else bag.append(0)

  # output is 1 for current tag and 0 for rest of other tags 
  output_row = list(output_empty)
  output_row[classes.index(doc[1])] = 1
  training.append([bag,output_row])

  # shuffling features 
  random.shuffle(training)
  train = np.array(training)

  # creating training list
  train_x = list(train[:,0])
  train_y = list(train[:,1])

# resetting under lying graph
tf.reset_default_graph()

# Building NN
net = tflearn.input_data(shape=[None,len(train_x[0])])
net = tflearn.fully_connected(net,10)
net = tflearn.fully_connected(net,10)
net = tflearn.fully_connected(net,len(train_y[0]),activation="softmax")
net = tflearn.regression(net)

# define model
model = tflearn.DNN(net,tensorboard_dir='tflearn_logs')

# start training
model.fit(train_x,train_y,n_epoch=2000,batch_size=8,show_metric=True)
model.save('model.tflearn')

import pickle 
pickle.dump( {'words':words , 'classes':classes , 'train_x':train_x , 'train_y':train_y},open('training_data','wb'))

# restoring all data structures 
data = pickle.load(open('training_data','rb'))
words = data['words']
classes = data['classes']
train_x = data['train_x']
train_y = data['train_y']

with open('intents.json') as json_data:
  intents = json.load(json_data)

# load model
model.load('./model.tflearn')

def clean_up_sentence(sentence):
  # tokenize
  sentence_words = nltk.word_tokenize(sentence)
  # stemming 
  sentence_words = [stemmer.stem(word.lower()) for word in sentence_words]
  return sentence_words

def bow(sentence,words,show_details=False):
  # tokenize patterns 
  sentence_words = clean_up_sentence(sentence)
  # Generate bag of words 
  bag = [0] * len(words)
  for s in sentence_words:
    for i,w in enumerate(words):
      if w == s:
        bag[i] = 1
        if show_details:
          print("found in bag: %s",w)
  return(np.array(bag))

# RESPONSE PROCESSOR 
ERROR_THRESHOLD = 0.30
def classify(sentence):
  # generate probability from model
  results = model.predict([bow(sentence,words)])[0]
  # filter out predictions 
  results = [[i,r] for i,r in enumerate(results) if r>ERROR_THRESHOLD]
  # sort by strength of probability
  results.sort(key=lambda x:x[1],reverse=True)
  return_list = []
  for r in results:
    return_list.append((classes[r[0]],r[1]))
  # return tuple of intent and probability
  return return_list

def response(sentence,userID='123',show_details=False):
  results = classify(sentence)

  if results:

    while results:
      for i in intents['intents']:
        if i['tag'] == results[0][0]:

          return print(random.choice(i['responses']))

      results.pop(0)

classify("What are your working hours?")

classify("What do you have for lunch")

response("Where can we locate you")

# Adding context to the conversation
context = {}
ERROR_THRESHOLD = 0.30
def classify(sentence):
  # generate probability from model
  results = model.predict([bow(sentence,words)])[0]
  # filter out predictions 
  results = [[i,r] for i,r in enumerate(results) if r>ERROR_THRESHOLD]
  # sort by strength of probability
  results.sort(key=lambda x:x[1],reverse=True)
  return_list = []
  for r in results:
    return_list.append((classes[r[0]],r[1]))
  # return tuple of intent and probability
  return return_list

def response(sentence,userID='123',show_details=False):
  results = classify(sentence)

  if results:

    while results:
      for i in intents['intents']:
        # find a tag matching the first result 
        if i['tag'] == results[0][0]:
          # set context for the intent if necessary 
          if "context_set" in i:
            if show_details: print("context: ",i['context_set'])
            context[userID] = i['context_set']
            # check if this intent is contextual and applies to this users conversation

            if not "context_set" in i or (userID in context and "context_filter" in i and i["context_filter"] == context["userID"]):
              if show_details: print('tag:',i['tag'])
            
          return print(random.choice(i['responses']))
            

          

      results.pop(0)

response("Hi there",show_details=True)

response("What is menu for today?")

response("What is your location?")

